{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af7b2d07"
      },
      "source": [
        "# N8n Workflow Scraper\n",
        "\n",
        "This project automates scraping n8n workflows, saving JSON locally, uploading to Google Drive, and updating a Google Sheet with workflow data and Drive links, all via a Gradio interface."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f41ab47a"
      },
      "source": [
        "!pip install selenium gspread google-api-python-client google-auth pyperclip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "fFWQJsSGK34H"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Complete Browser Automation for n8n SEO Workflow Scraping\n",
        "This script implements your exact workflow requirements:\n",
        "1. Navigate to search results page\n",
        "2. Click on workflow links (including pagination)\n",
        "3. Extract workflow data (JSON, title, description, URL)\n",
        "4. Save JSON locally\n",
        "5. Upload JSON to Google Drive folder\n",
        "6. Get Google Drive shareable link\n",
        "7. Update Google Sheet with workflow data and Drive link\n",
        "\"\"\"\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException, ElementClickInterceptedException\n",
        "import time\n",
        "import csv\n",
        "import json\n",
        "import re\n",
        "import os\n",
        "import pyperclip\n",
        "\n",
        "# Import Google API libraries\n",
        "from googleapiclient.discovery import build\n",
        "from googleapiclient.http import MediaFileUpload\n",
        "from googleapiclient.errors import HttpError\n",
        "import gspread\n",
        "from google.auth import default # Assuming google.colab.auth.authenticate_user() is run before this script\n",
        "\n",
        "class N8nBrowserAutomation:\n",
        "    def __init__(self, headless=True, google_sheet_url=None, google_drive_folder_id=None, start_url=None):\n",
        "        \"\"\"Initialize the browser automation and Google API clients\"\"\"\n",
        "        self.setup_chrome_driver(headless)\n",
        "        self.workflows = []\n",
        "        self.processed_urls = set()\n",
        "        self.start_url = start_url # Store the start URL\n",
        "\n",
        "        # Create output directory for local JSONs\n",
        "        os.makedirs(\"automated_json_workflows\", exist_ok=True)\n",
        "\n",
        "        # Google Sheet and Drive setup\n",
        "        self.google_sheet_url = google_sheet_url\n",
        "        self.google_drive_folder_id = google_drive_folder_id\n",
        "        self.sheet = None\n",
        "        self.worksheet = None\n",
        "        self.drive_service = None\n",
        "        self.google_api_creds = None # To store credentials from google.auth.default()\n",
        "\n",
        "        if self.google_sheet_url or self.google_drive_folder_id:\n",
        "             try:\n",
        "                 # Get credentials from google.colab.auth.authenticate_user()\n",
        "                 self.google_api_creds, _ = default()\n",
        "                 print(\"âœ“ Google API credentials obtained.\")\n",
        "\n",
        "                 if self.google_sheet_url:\n",
        "                     # Initialize gspread client\n",
        "                     self.gc = gspread.authorize(self.google_api_creds)\n",
        "                     self.sheet = self.gc.open_by_url(self.google_sheet_url)\n",
        "                     self.worksheet = self.sheet.get_worksheet(0) # Assuming first sheet\n",
        "                     print(f\"âœ“ Connected to Google Sheet: {self.google_sheet_url}\")\n",
        "\n",
        "                     # Prepare Sheet header if empty\n",
        "                     if not self.worksheet.get_all_values():\n",
        "                          header = [\"title\", \"description\", \"url\", \"json_file_name\", \"drive link\"]\n",
        "                          self.worksheet.append_row(header)\n",
        "                          print(\"âœ“ Added header row to Google Sheet.\")\n",
        "\n",
        "                 if self.google_drive_folder_id:\n",
        "                     # Initialize Google Drive API client\n",
        "                     self.drive_service = build('drive', 'v3', credentials=self.google_api_creds)\n",
        "                     print(f\"âœ“ Google Drive API service initialized for folder ID: {self.google_drive_folder_id}\")\n",
        "\n",
        "             except Exception as e:\n",
        "                 print(f\"âœ— Failed to initialize Google API clients: {e}\")\n",
        "                 print(\"Please ensure google.colab.auth.authenticate_user() was run successfully and APIs are enabled.\")\n",
        "\n",
        "\n",
        "        # Workflow counts\n",
        "        self.total_found = 0\n",
        "        self.successful_extractions = 0\n",
        "        self.successful_uploads = 0\n",
        "        self.successful_sheet_updates = 0\n",
        "\n",
        "\n",
        "    def setup_chrome_driver(self, headless=True):\n",
        "        \"\"\"Setup Chrome WebDriver with options\"\"\"\n",
        "        chrome_options = Options()\n",
        "\n",
        "        if headless:\n",
        "            chrome_options.add_argument(\"--headless\")\n",
        "\n",
        "        chrome_options.add_argument(\"--no-sandbox\")\n",
        "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "        chrome_options.add_argument(\"--disable-gpu\")\n",
        "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
        "        chrome_options.add_argument(\"--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36\")\n",
        "\n",
        "        # Disable notifications and popups\n",
        "        prefs = {\n",
        "            \"profile.default_content_setting_values.notifications\": 2,\n",
        "            \"profile.default_content_settings.popups\": 0,\n",
        "            \"profile.managed_default_content_settings.images\": 2  # Don't load images for speed\n",
        "        }\n",
        "        chrome_options.add_experimental_option(\"prefs\", prefs)\n",
        "\n",
        "        try:\n",
        "            self.driver = webdriver.Chrome(options=chrome_options)\n",
        "            self.wait = WebDriverWait(self.driver, 20) # Increased wait time\n",
        "            print(\"âœ“ Chrome WebDriver initialized successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Failed to initialize Chrome WebDriver: {e}\")\n",
        "            print(\"Make sure Chrome and ChromeDriver are installed\")\n",
        "            raise\n",
        "\n",
        "    def sanitize_filename(self, filename):\n",
        "        \"\"\"Clean filename for safe file system usage\"\"\"\n",
        "        filename = re.sub(r'[<>:\"/\\\\|?*]', '_', filename)\n",
        "        filename = re.sub(r'\\s+', '_', filename)\n",
        "        if len(filename) > 100:\n",
        "            filename = filename[:80] + \"...\" + filename[-17:]\n",
        "        return filename.strip()\n",
        "\n",
        "    def navigate_to_search_results(self):\n",
        "        \"\"\"Navigate to the SEO search results page\"\"\"\n",
        "        url = self.start_url if self.start_url else \"https://n8n.io/workflows/categories/crypto-trading/\" # Use start_url if provided\n",
        "        print(f\"Navigating to: {url}\")\n",
        "\n",
        "        try:\n",
        "            self.driver.get(url)\n",
        "            time.sleep(3)  # Wait for page to load\n",
        "\n",
        "            print(f\"âœ“ Successfully loaded: {self.driver.title}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Failed to navigate to search results: {e}\")\n",
        "            return False\n",
        "\n",
        "    def extract_workflow_links(self):\n",
        "        \"\"\"Extract workflow links from the current page\"\"\"\n",
        "        workflow_links = []\n",
        "\n",
        "        try:\n",
        "            # Find all links that match workflow pattern\n",
        "            links = self.driver.find_elements(By.CSS_SELECTOR, \"a[href*='/workflows/']\")\n",
        "\n",
        "            for link in links:\n",
        "                href = link.get_attribute('href')\n",
        "                if href and re.match(r'.*/workflows/\\d+.*', href):\n",
        "                    if href not in workflow_links and href not in self.processed_urls:\n",
        "                        workflow_links.append(href)\n",
        "\n",
        "            # print(f\"Found {len(workflow_links)} new workflow links on this page\") # Suppress this print inside the loop for cleaner output\n",
        "            return workflow_links\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting workflow links: {e}\")\n",
        "            return []\n",
        "\n",
        "    # Removed click_use_for_free_button()\n",
        "\n",
        "    # Removed click_copy_to_clipboard()\n",
        "\n",
        "    def extract_workflow_json(self):\n",
        "        \"\"\"Extract workflow JSON directly from <n8n-demo> element\"\"\"\n",
        "        try:\n",
        "            n8n_demo = self.wait.until(\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"n8n-demo\"))\n",
        "            )\n",
        "            workflow_json = n8n_demo.get_attribute(\"workflow\")\n",
        "            if workflow_json and len(workflow_json) > 100:\n",
        "                print(f\"âœ“ Successfully extracted workflow JSON ({len(workflow_json)} characters)\")\n",
        "                return workflow_json\n",
        "            else:\n",
        "                print(\"âœ— Workflow JSON is empty or too short\")\n",
        "                return None\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error extracting workflow JSON: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "    def extract_workflow_metadata(self):\n",
        "        \"\"\"Extract title, description, and URL from current page\"\"\"\n",
        "        try:\n",
        "            # Extract title from og:title meta tag\n",
        "            title_element = self.driver.find_element(By.CSS_SELECTOR, \"meta[property='og:title']\")\n",
        "            title = title_element.get_attribute(\"content\")\n",
        "\n",
        "            # Extract description from og:description meta tag\n",
        "            try:\n",
        "                desc_element = self.driver.find_element(By.CSS_SELECTOR, \"meta[property='og:description']\")\n",
        "                description = desc_element.get_attribute(\"content\")\n",
        "            except NoSuchElementException:\n",
        "                description = title  # Fallback to title\n",
        "\n",
        "            # Extract URL from og:url meta tag\n",
        "            try:\n",
        "                url_element = self.driver.find_element(By.CSS_SELECTOR, \"meta[property='og:url']\")\n",
        "                url = url_element.get_attribute(\"content\")\n",
        "            except NoSuchElementException:\n",
        "                url = self.driver.current_url  # Fallback to current URL\n",
        "\n",
        "            return {\n",
        "                'title': title,\n",
        "                'description': description,\n",
        "                'url': url\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting metadata: {e}\")\n",
        "            return {\n",
        "                'title': 'Unknown Title',\n",
        "                'description': 'No description available',\n",
        "                'url': self.driver.current_url\n",
        "            }\n",
        "\n",
        "    def save_json_file_locally(self, json_content, title):\n",
        "        \"\"\"Save JSON content to file locally with sanitized title\"\"\"\n",
        "        if not json_content:\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            # Validate JSON content\n",
        "            json_data = json.loads(json_content)\n",
        "\n",
        "            filename = self.sanitize_filename(title) + \".json\"\n",
        "            filepath = os.path.join(\"automated_json_workflows\", filename)\n",
        "\n",
        "            with open(filepath, 'w', encoding='utf-8') as f:\n",
        "                json.dump(json_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "            return filename\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"âœ— Invalid JSON content for {title}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving JSON file locally for {title}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def upload_json_to_drive(self, file_path):\n",
        "        \"\"\"Uploads a file to Google Drive folder and returns its shareable link.\"\"\"\n",
        "        if not self.drive_service or not self.google_drive_folder_id:\n",
        "            print(\"âœ— Google Drive service not initialized or folder ID not provided.\")\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            file_name = os.path.basename(file_path)\n",
        "            media = MediaFileUpload(file_path, resumable=True)\n",
        "\n",
        "            file_metadata = {'name': file_name, 'parents': [self.google_drive_folder_id]}\n",
        "\n",
        "            # Upload the file\n",
        "            uploaded_file = self.drive_service.files().create(\n",
        "                body=file_metadata,\n",
        "                media_body=media,\n",
        "                fields='id, webViewLink' # Request file ID and shareable link\n",
        "            ).execute()\n",
        "\n",
        "            file_id = uploaded_file.get('id')\n",
        "            shareable_link = uploaded_file.get('webViewLink')\n",
        "\n",
        "            print(f\"âœ“ Uploaded '{file_name}' to Drive (ID: {file_id}).\")\n",
        "\n",
        "            # Make the file shareable (optional, based on user's statement \"all are public\")\n",
        "            try:\n",
        "                self.drive_service.permissions().create(\n",
        "                    fileId=file_id,\n",
        "                    body={'type': 'anyone', 'role': 'reader'},\n",
        "                    fields='id'\n",
        "                ).execute()\n",
        "                print(f\"âœ“ File '{file_name}' is now shareable.\")\n",
        "            except Exception as share_error:\n",
        "                print(f\"Warning: Could not make '{file_name}' shareable: {share_error}\")\n",
        "\n",
        "\n",
        "            self.successful_uploads += 1\n",
        "            return shareable_link\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— An error occurred during Drive upload for '{os.path.basename(file_path)}': {e}\")\n",
        "            return None\n",
        "\n",
        "    def update_google_sheet(self, data_row):\n",
        "        \"\"\"Appends a row of data to the Google Sheet.\"\"\"\n",
        "        if not self.worksheet:\n",
        "            print(\"âœ— Google Sheet not initialized.\")\n",
        "            return False\n",
        "\n",
        "        try:\n",
        "            self.worksheet.append_row(data_row)\n",
        "            print(f\"âœ“ Appended data to Google Sheet for: {data_row[0]}\")\n",
        "            self.successful_sheet_updates += 1\n",
        "            return True\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— An error occurred while updating Google Sheet for: {data_row[0]}: {e}\")\n",
        "            return False\n",
        "\n",
        "\n",
        "    def process_single_workflow(self, workflow_url):\n",
        "        \"\"\"Processes a single workflow: scrapes, saves locally, uploads to Drive, updates Sheet.\"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"Processing: {workflow_url}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        workflow_result = {\n",
        "            'title': 'N/A',\n",
        "            'description': 'N/A',\n",
        "            'url': workflow_url, # Default to current URL\n",
        "            'json_file_name': 'N/A',\n",
        "            'drive_link': 'N/A'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Step 1: Navigate to workflow page\n",
        "            self.driver.get(workflow_url)\n",
        "            time.sleep(2)\n",
        "\n",
        "            # Step 2: Extract metadata (title, description, URL)\n",
        "            metadata = self.extract_workflow_metadata()\n",
        "            workflow_result['title'] = metadata['title']\n",
        "            workflow_result['description'] = metadata['description']\n",
        "            workflow_result['url'] = metadata['url']\n",
        "            print(f\"Title: {workflow_result['title']}\")\n",
        "            print(f\"URL: {workflow_result['url']}\")\n",
        "\n",
        "\n",
        "            # Step 3: Directly extract JSON from <n8n-demo>\n",
        "            json_content = self.extract_workflow_json()\n",
        "\n",
        "            if not json_content:\n",
        "                print(\"âœ— Failed to extract workflow JSON.\")\n",
        "                # Still record metadata even if JSON extraction fails\n",
        "                if self.worksheet:\n",
        "                    self.update_google_sheet([\n",
        "                        workflow_result['title'],\n",
        "                        workflow_result['description'],\n",
        "                        workflow_result['url'],\n",
        "                        workflow_result['json_file_name'],\n",
        "                        workflow_result['drive_link']\n",
        "                    ])\n",
        "                return workflow_result # Return result even if JSON extraction failed\n",
        "\n",
        "\n",
        "            self.successful_extractions += 1\n",
        "\n",
        "            # Step 4: Save JSON file locally\n",
        "            json_filename = self.save_json_file_locally(json_content, workflow_result['title'])\n",
        "\n",
        "            if not json_filename:\n",
        "                print(\"âœ— Failed to save JSON file locally.\")\n",
        "                 # Still record metadata even if local save fails\n",
        "                if self.worksheet:\n",
        "                    self.update_google_sheet([\n",
        "                        workflow_result['title'],\n",
        "                        workflow_result['description'],\n",
        "                        workflow_result['url'],\n",
        "                        workflow_result['json_file_name'],\n",
        "                        workflow_result['drive_link']\n",
        "                    ])\n",
        "                return workflow_result # Return result even if local save failed\n",
        "\n",
        "\n",
        "            workflow_result['json_file_name'] = json_filename\n",
        "            local_file_path = os.path.join(\"automated_json_workflows\", json_filename)\n",
        "            print(f\"âœ“ Saved JSON file locally: {local_file_path}\")\n",
        "\n",
        "\n",
        "            # Step 5 & 6: Upload JSON to Google Drive and get link\n",
        "            if self.drive_service and self.google_drive_folder_id:\n",
        "                 drive_link = self.upload_json_to_drive(local_file_path)\n",
        "                 workflow_result['drive_link'] = drive_link if drive_link else 'N/A'\n",
        "            else:\n",
        "                print(\"Skipping Google Drive upload: Service not initialized or folder ID missing.\")\n",
        "\n",
        "\n",
        "            # Step 7: Update Google Sheet\n",
        "            if self.worksheet:\n",
        "                 self.update_google_sheet([\n",
        "                     workflow_result['title'],\n",
        "                     workflow_result['description'],\n",
        "                     workflow_result['url'],\n",
        "                     workflow_result['json_file_name'],\n",
        "                     workflow_result['drive_link']\n",
        "                 ])\n",
        "            else:\n",
        "                print(\"Skipping Google Sheet update: Service not initialized.\")\n",
        "\n",
        "\n",
        "            return workflow_result\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âœ— Error processing workflow {workflow_url}: {e}\")\n",
        "             # Attempt to log even failed workflows to the sheet if possible\n",
        "            if self.worksheet:\n",
        "                 try:\n",
        "                      self.update_google_sheet([\n",
        "                         workflow_result.get('title', 'Error'),\n",
        "                         workflow_result.get('description', f'Error: {e}'),\n",
        "                         workflow_result.get('url', workflow_url),\n",
        "                         workflow_result.get('json_file_name', 'N/A'),\n",
        "                         workflow_result.get('drive_link', 'N/A')\n",
        "                      ])\n",
        "                 except Exception as sheet_log_error:\n",
        "                      print(f\"âœ— Failed to log error to sheet: {sheet_log_error}\")\n",
        "            return None\n",
        "        finally:\n",
        "            # Mark as processed\n",
        "            self.processed_urls.add(workflow_url)\n",
        "\n",
        "\n",
        "    def click_load_more_templates(self):\n",
        "        \"\"\"Click 'Load more templates' button if available\"\"\"\n",
        "        try:\n",
        "            load_more_selector = \"button.search-results-load-more-btn\"\n",
        "\n",
        "            # Wait for the load more button to be clickable\n",
        "            load_more_button = WebDriverWait(self.driver, 30).until( # Increased timeout\n",
        "                EC.element_to_be_clickable((By.CSS_SELECTOR, load_more_selector))\n",
        "            )\n",
        "\n",
        "            # Scroll to button if needed\n",
        "            self.driver.execute_script(\"arguments[0].scrollIntoView(true);\", load_more_button)\n",
        "            time.sleep(1)\n",
        "\n",
        "            # Click the button and wait for new content to load\n",
        "            self.driver.execute_script(\"arguments[0].click();\", load_more_button) # Use JavaScript click for reliability\n",
        "            print(\"âœ“ Clicked 'Load more templates' button\")\n",
        "            time.sleep(5)  # Wait for new content to load\n",
        "\n",
        "            # After clicking, wait for new workflow links to be present\n",
        "            WebDriverWait(self.driver, 20).until( # Wait for at least one new link\n",
        "                EC.presence_of_element_located((By.CSS_SELECTOR, \"a[href*='/workflows/']\"))\n",
        "            )\n",
        "            return True\n",
        "\n",
        "        except TimeoutException:\n",
        "             print(\"Timed out waiting for 'Load more templates' button or button not clickable\")\n",
        "             return False\n",
        "        except NoSuchElementException:\n",
        "            print(\"No 'Load more templates' button found\")\n",
        "            return False\n",
        "        except Exception as e:\n",
        "            print(f\"Error clicking load more button: {e}\")\n",
        "            return False\n",
        "\n",
        "    def run_complete_automation(self):\n",
        "        \"\"\"Run the complete automation: scrape, upload, and update Sheet.\"\"\"\n",
        "        print(\"=\"*80)\n",
        "        print(\"STARTING N8n SEO WORKFLOW AUTOMATION\")\n",
        "        print(\"Following process:\")\n",
        "        print(\"1. Navigate to search results page & load all links.\")\n",
        "        print(\"2. For each workflow: scrape data, save JSON locally, upload to Drive, update Sheet.\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        if not (self.google_sheet_url or self.google_drive_folder_id):\n",
        "            print(\"Error: Google Sheet URL or Drive Folder ID not provided. Cannot proceed with Google integration.\")\n",
        "            print(\"Please provide at least one of these when initializing N8nBrowserAutomation.\")\n",
        "            return\n",
        "\n",
        "\n",
        "        try:\n",
        "            # Step 1: Initial navigation to search results\n",
        "            if not self.navigate_to_search_results():\n",
        "                 print(\"Failed to navigate to initial search results page. Aborting.\")\n",
        "                 return\n",
        "\n",
        "            # Step 2: Extract all workflow links by loading more pages\n",
        "            all_workflow_links = set()\n",
        "            previous_count = 0\n",
        "            page_num = 1\n",
        "\n",
        "            while True:\n",
        "                print(f\"\\nðŸ”„ Loading and extracting links from page {page_num}...\")\n",
        "                current_links = self.extract_workflow_links()\n",
        "                newly_found_links = [link for link in current_links if link not in all_workflow_links]\n",
        "\n",
        "                if not newly_found_links:\n",
        "                    print(f\"No *new* workflow links found on page {page_num}.\")\n",
        "                    # If no *new* links are found, and we couldn't load more last time, break\n",
        "                    print(\"Attempting to click 'Load more templates' to check for more pages...\")\n",
        "                    if not self.click_load_more_templates():\n",
        "                        print(\"No more pages to load or failed to load more templates.\")\n",
        "                        break\n",
        "                    else:\n",
        "                        # If load more was clicked, wait for new content to render and try extracting links again\n",
        "                        time.sleep(5) # Give the page time to render new links\n",
        "                        page_num += 1\n",
        "                        previous_count = len(all_workflow_links) # Update previous count after loading more\n",
        "                        continue # Continue the loop to extract links from the new content\n",
        "\n",
        "\n",
        "                all_workflow_links.update(newly_found_links)\n",
        "                print(f\"Found {len(newly_found_links)} new links. Total links collected: {len(all_workflow_links)}\")\n",
        "\n",
        "                # If the number of links didn't increase significantly after attempting to load more, break\n",
        "                # This helps prevent infinite loops if click_load_more_templates succeeds but no new links appear\n",
        "                if len(all_workflow_links) <= previous_count + 5 and page_num > 1: # Check if at least 5 new links were added (adjust threshold as needed)\n",
        "                     print(\"Warning: Few or no new links found after clicking 'Load more'. Assuming end of results or issue.\")\n",
        "                     # Try one more click just in case, then break if still no significant change\n",
        "                     if not self.click_load_more_templates():\n",
        "                          print(\"No significant new links loaded and cannot load more. Assuming end of results.\")\n",
        "                          break\n",
        "                     else:\n",
        "                          # If another click worked, continue\n",
        "                          time.sleep(5)\n",
        "                          page_num += 1\n",
        "                          previous_count = len(all_workflow_links)\n",
        "                          continue\n",
        "\n",
        "\n",
        "                previous_count = len(all_workflow_links)\n",
        "\n",
        "                # Try clicking 'Load more templates' for the next page\n",
        "                print(f\"\\nðŸ”„ Attempting to load more templates for page {page_num + 1}...\")\n",
        "                if not self.click_load_more_templates():\n",
        "                    print(\"No more pages to load\")\n",
        "                    break\n",
        "\n",
        "                page_num += 1\n",
        "\n",
        "                # Safety break to avoid excessive loops\n",
        "                if page_num > 100: # Increased safety limit for pages\n",
        "                    print(\"Reached maximum page limit for loading links.\")\n",
        "                    break\n",
        "\n",
        "\n",
        "            self.total_found = len(all_workflow_links)\n",
        "            print(f\"\\nFinished collecting links. Total unique workflows found: {self.total_found}\")\n",
        "\n",
        "            # Step 3: Process each collected workflow link (scrape, save, upload, update sheet)\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"STARTING WORKFLOW PROCESSING, UPLOAD, AND SHEET UPDATE\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            if not all_workflow_links:\n",
        "                 print(\"No workflows to process.\")\n",
        "                 return\n",
        "\n",
        "\n",
        "            for i, workflow_url in enumerate(list(all_workflow_links), 1):\n",
        "                 # Check if we have already processed this URL in case of multiple runs or partial runs\n",
        "                 # This check is now less critical as we filter processed_urls when collecting links\n",
        "                 # but can be kept as an extra safeguard if needed.\n",
        "                 # if workflow_url in self.processed_urls:\n",
        "                 #     print(f\"\\nSkipping already processed workflow: {workflow_url}\")\n",
        "                 #     continue\n",
        "\n",
        "                 print(f\"\\n[{i}/{len(all_workflow_links)}] Processing workflow {self.successful_extractions + 1}...\")\n",
        "\n",
        "                 # Process the workflow, which now includes saving JSON, uploading, and updating the sheet\n",
        "                 result = self.process_single_workflow(workflow_url)\n",
        "                 if result:\n",
        "                     # The result is already logged to the sheet within process_single_workflow\n",
        "                     pass\n",
        "\n",
        "\n",
        "                 # Brief pause between workflows\n",
        "                 time.sleep(1)\n",
        "\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(\"AUTOMATION COMPLETE!\")\n",
        "            print(f\"Total unique workflows identified: {self.total_found}\")\n",
        "            print(f\"Successful JSON extractions: {self.successful_extractions}\")\n",
        "            print(f\"Successful Drive uploads: {self.successful_uploads}\")\n",
        "            print(f\"Successful Sheet updates: {self.successful_sheet_updates}\")\n",
        "            print(f\"JSON files saved locally in: automated_json_workflows/\")\n",
        "            if self.google_drive_folder_id:\n",
        "                 print(f\"JSON files uploaded to Google Drive folder ID: {self.google_drive_folder_id}\")\n",
        "            if self.google_sheet_url:\n",
        "                 print(f\"Results updated in Google Sheet: {self.google_sheet_url}\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "\n",
        "        except KeyboardInterrupt:\n",
        "            print(\"\\nâš ï¸  Automation interrupted by user\")\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ Unexpected error during automation: {e}\")\n",
        "        finally:\n",
        "            self.cleanup()\n",
        "\n",
        "    def save_results_to_csv(self):\n",
        "        \"\"\"Removed: Results are now updated directly in Google Sheet.\"\"\"\n",
        "        pass # This method is no longer used\n",
        "\n",
        "\n",
        "    def cleanup(self):\n",
        "        \"\"\"Clean up resources\"\"\"\n",
        "        if hasattr(self, 'driver'):\n",
        "            self.driver.quit()\n",
        "            print(\"âœ“ Browser closed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a24a6f0"
      },
      "source": [
        "!pip install gradio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "31cbff3b",
        "outputId": "82160bfe-7713-40bf-c2ea-2f44a6527158"
      },
      "source": [
        "import gradio as gr\n",
        "from google.colab import auth\n",
        "from google.auth import default\n",
        "\n",
        "# Authenticate user for Google API access\n",
        "# This will open a new tab for authentication\n",
        "try:\n",
        "    auth.authenticate_user()\n",
        "    print('Authenticated')\n",
        "except Exception as e:\n",
        "    print(f\"Authentication failed: {e}\")\n",
        "    print(\"Please ensure you have enabled the Google Drive and Google Sheets APIs for your project.\")\n",
        "\n",
        "\n",
        "def scrape_workflows_gradio(folder_link: str, sheet_link: str, start_url: str) -> str:\n",
        "    \"\"\"\n",
        "    Initiates the N8n workflow scraping automation with Gradio inputs.\n",
        "\n",
        "    Args:\n",
        "        folder_link: The Google Drive folder ID.\n",
        "        sheet_link: The Google Sheet URL.\n",
        "        start_url: The starting URL for scraping.\n",
        "\n",
        "    Returns:\n",
        "        A string indicating the status of the scraping process.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Instantiate the automation class with inputs from Gradio\n",
        "        automation = N8nBrowserAutomation(\n",
        "            headless=True, # Run in headless mode for Gradio app\n",
        "            google_sheet_url=sheet_link,\n",
        "            google_drive_folder_id=folder_link, # Use folder_link as the ID\n",
        "            start_url=start_url # Pass the start URL\n",
        "        )\n",
        "\n",
        "        # Run the automation\n",
        "        automation.run_complete_automation()\n",
        "\n",
        "        return \"Scraping process initiated and completed (check console for details).\"\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred during scraping: {e}\"\n",
        "\n",
        "# Define the Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=scrape_workflows_gradio,\n",
        "    inputs=[\n",
        "        gr.Textbox(label=\"Google Drive Folder ID\"),\n",
        "        gr.Textbox(label=\"Google Sheet URL\"),\n",
        "        gr.Textbox(label=\"Starting URL for Scraping\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"N8n Workflow Scraper\",\n",
        "    description=\"Enter your Google Drive Folder ID, Google Sheet URL, and the starting URL to scrape N8n workflows and save them to Drive and Sheet.\"\n",
        ")\n",
        "\n",
        "# Launch the Gradio app\n",
        "interface.launch(debug=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authenticated\n",
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a797447cc9ca23d384.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://a797447cc9ca23d384.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Chrome WebDriver initialized successfully\n",
            "âœ“ Google API credentials obtained.\n",
            "âœ“ Connected to Google Sheet: https://docs.google.com/spreadsheets/d/1ZymWkU7v8R0UhMyjfTZcvU1cn8zfS0bJVuQk4mdjpTk/edit\n",
            "âœ“ Google Drive API service initialized for folder ID: 1OCattfy5XzdV2Ap5xUL1ibwcDFTyTemu\n",
            "================================================================================\n",
            "STARTING N8n SEO WORKFLOW AUTOMATION\n",
            "Following process:\n",
            "1. Navigate to search results page & load all links.\n",
            "2. For each workflow: scrape data, save JSON locally, upload to Drive, update Sheet.\n",
            "================================================================================\n",
            "Navigating to: https://n8n.io/workflows/categories/SEO/\n",
            "âœ“ Successfully loaded: \n",
            "\n",
            "ðŸ”„ Loading and extracting links from page 1...\n",
            "No *new* workflow links found on page 1.\n",
            "Attempting to click 'Load more templates' to check for more pages...\n",
            "Timed out waiting for 'Load more templates' button or button not clickable\n",
            "No more pages to load or failed to load more templates.\n",
            "\n",
            "Finished collecting links. Total unique workflows found: 0\n",
            "\n",
            "================================================================================\n",
            "STARTING WORKFLOW PROCESSING, UPLOAD, AND SHEET UPDATE\n",
            "================================================================================\n",
            "No workflows to process.\n",
            "âœ“ Browser closed\n",
            "âœ“ Chrome WebDriver initialized successfully\n",
            "âœ“ Google API credentials obtained.\n",
            "âœ“ Connected to Google Sheet: https://docs.google.com/spreadsheets/d/1ZymWkU7v8R0UhMyjfTZcvU1cn8zfS0bJVuQk4mdjpTk/edit\n",
            "âœ“ Google Drive API service initialized for folder ID: 1OCattfy5XzdV2Ap5xUL1ibwcDFTyTemu\n",
            "================================================================================\n",
            "STARTING N8n SEO WORKFLOW AUTOMATION\n",
            "Following process:\n",
            "1. Navigate to search results page & load all links.\n",
            "2. For each workflow: scrape data, save JSON locally, upload to Drive, update Sheet.\n",
            "================================================================================\n",
            "Navigating to: https://n8n.io/workflows/categories/crm/\n",
            "âœ“ Successfully loaded: Discover 269 CRM Automation Workflows from the n8n's Community\n",
            "\n",
            "ðŸ”„ Loading and extracting links from page 1...\n",
            "Found 20 new links. Total links collected: 20\n",
            "\n",
            "ðŸ”„ Attempting to load more templates for page 2...\n",
            "âœ“ Clicked 'Load more templates' button\n",
            "\n",
            "ðŸ”„ Loading and extracting links from page 2...\n",
            "Found 10 new links. Total links collected: 30\n",
            "\n",
            "ðŸ”„ Attempting to load more templates for page 3...\n",
            "âœ“ Clicked 'Load more templates' button\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://a797447cc9ca23d384.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}